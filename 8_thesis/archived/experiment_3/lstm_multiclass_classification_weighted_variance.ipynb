{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import json\n",
    "import wandb\n",
    "from wandb.integration.keras import WandbMetricsLogger, WandbEvalCallback, WandbModelCheckpoint\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import classification_report"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "Tokenizer = tf.keras.preprocessing.text.Tokenizer\n",
    "pad_sequences = tf.keras.utils.pad_sequences\n",
    "Sequential = tf.keras.models.Sequential\n",
    "Dense = tf.keras.layers.Dense\n",
    "Embedding = tf.keras.layers.Embedding\n",
    "LSTM = tf.keras.layers.LSTM\n",
    "SpatialDropout1D = tf.keras.layers.SpatialDropout1D\n",
    "EarlyStopping = tf.keras.callbacks.EarlyStopping\n",
    "to_categorical = tf.keras.utils.to_categorical\n",
    "Dropout = tf.keras.layers.Dropout"
   ],
   "id": "e3d99186459d4cdd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "config_data = json.load(open('../config.json'))\n",
    "HF_TOKEN = config_data['HF_TOKEN']\n",
    "WANDB_TOKEN = config_data['WANDB_TOKEN']\n",
    "\n",
    "wandb.login(key=WANDB_TOKEN)\n",
    "\n",
    "run = wandb.init(\n",
    "    project='wz_experimental',\n",
    "    config={\"model_name\": \"LSTM\"}\n",
    ")"
   ],
   "id": "ce595aea75006654",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "id_to_label_mapping = {0: 'A1', 1: 'A2', 2: 'B1', 3: 'B2', 4: 'C1', 5: 'C2'}\n",
    "label_to_id_mapping = {'A1': 0, 'A2': 1, 'B1': 2, 'B2': 3, 'C1': 4, 'C2': 5}\n",
    "cefr_levels = [\"A1\", \"A2\", \"B1\", \"B2\", \"C1\", \"C2\"]\n",
    "\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = set(stopwords.words('english'))"
   ],
   "id": "c20a5ab575d56ebe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "config = wandb.config\n",
    "\n",
    "config.maxlen = 1000\n",
    "config.vocab_size = 30000\n",
    "config.embedding_dims = 200\n",
    "config.epochs = 7\n",
    "config.hidden_dims = 200\n",
    "config.batch_size = 32"
   ],
   "id": "cba2b489b4da6e92",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train = pd.read_csv('../datasets/quotes/quotes_train.csv')\n",
    "test = pd.read_csv('../datasets/quotes/quotes_test.csv')\n",
    "\n",
    "print(train['level'].value_counts())\n",
    "print(test['level'].value_counts())\n",
    "\n",
    "train = train.sample(frac=1).reset_index(drop=True)\n",
    "test = test.sample(frac=1).reset_index(drop=True)"
   ],
   "id": "28fd532cbd3935d1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text) \n",
    "    text = BAD_SYMBOLS_RE.sub('', text)  \n",
    "    text = text.replace('x', '')\n",
    "    \n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS)\n",
    "    return text"
   ],
   "id": "c3a1cd7bf19135e1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train['cleaned_text'] = train['text'].apply(clean_text)\n",
    "test['cleaned_text'] = test['text'].apply(clean_text)\n",
    "\n",
    "train['cleaned_text'] = train['cleaned_text'].str.replace('\\d+', '')\n",
    "test['cleaned_text'] = test['cleaned_text'].str.replace('\\d+', '')"
   ],
   "id": "2e428f6df4ae6e77",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "tokenizer = Tokenizer(num_words=config.vocab_size, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "tokenizer.fit_on_texts( pd.concat([train['cleaned_text'], test['cleaned_text']], ignore_index=True).values)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ],
   "id": "1cc2ca30e7fdb218",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "X_train_raw = train['cleaned_text']\n",
    "Y_train_raw = train['label']\n",
    "X_test_raw = test['cleaned_text']\n",
    "Y_test_raw = test['label']\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train_raw.values)\n",
    "X_train = pad_sequences(X_train, maxlen=config.maxlen)\n",
    "\n",
    "Y_train = pd.get_dummies(Y_train_raw).values\n",
    "\n",
    "\n",
    "X_test = tokenizer.texts_to_sequences(X_test_raw.values)\n",
    "X_test = pad_sequences(X_test, maxlen=config.maxlen)\n",
    "\n",
    "Y_test = pd.get_dummies(Y_test_raw).values\n",
    "\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)"
   ],
   "id": "bec13ed2ec19e6e0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(config.vocab_size, config.embedding_dims))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(config.hidden_dims, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(6, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ],
   "id": "2fbfd300d32ac95d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class WandbClfEvalCallback(WandbEvalCallback):\n",
    "    def __init__(self, validation_data, data_table_columns, pred_table_columns):\n",
    "        super().__init__(data_table_columns, pred_table_columns)\n",
    "\n",
    "        self.x = validation_data[0]\n",
    "        self.y = validation_data[1]\n",
    "\n",
    "    def add_ground_truth(self, logs=None):\n",
    "        for idx, (text, label) in enumerate(zip(self.x, self.y)):\n",
    "            self.data_table.add_data(idx, text, label)\n",
    "\n",
    "    def add_model_predictions(self, epoch, logs=None):\n",
    "        preds = self.model.predict(self.x, verbose=0)\n",
    "        preds = tf.argmax(preds, axis=-1)\n",
    "\n",
    "        data_table_ref = self.data_table_ref\n",
    "        table_idxs = data_table_ref.get_index()\n",
    "\n",
    "        for idx in table_idxs:\n",
    "            pred = preds[idx]\n",
    "            self.pred_table.add_data(\n",
    "                epoch,\n",
    "                data_table_ref.data[idx][0],\n",
    "                data_table_ref.data[idx][1],\n",
    "                data_table_ref.data[idx][2],\n",
    "                pred,\n",
    "            )\n",
    "\n",
    "history = model.fit(\n",
    "    X_train,Y_train,\n",
    "    epochs=config.epochs,\n",
    "    batch_size=config.batch_size,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001), WandbMetricsLogger(),\n",
    "               WandbClfEvalCallback(\n",
    "                   validation_data=(X_train, Y_train),\n",
    "                   data_table_columns=[\"idx\", \"text\", \"label\"],\n",
    "                   pred_table_columns=[\"epoch\", \"idx\", \"text\", \"label\", \"pred\"],\n",
    "               )]\n",
    ")"
   ],
   "id": "1da81c5be4145466",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "accuracy = model.evaluate(X_test,Y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accuracy[0],accuracy[1]))"
   ],
   "id": "72a752711853af3c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.title('Loss')\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "wandb.log({\"Loss\": plt})"
   ],
   "id": "5092db620c3da134",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.title('Accuracy')\n",
    "plt.plot(history.history['accuracy'], label='train')\n",
    "plt.plot(history.history['val_accuracy'], label='test')\n",
    "plt.legend()\n",
    "wandb.log({\"Accuracy\": plt})"
   ],
   "id": "e6f80c0b100eb9af",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import datetime \n",
    "\n",
    "def make_predictions(p_model, p_test):\n",
    "    print(f'Started prediction at {datetime.datetime.now()}')\n",
    "    for index, row in p_test.iterrows():\n",
    "        sentence = row['cleaned_text']\n",
    "        pred_sentence = tokenizer.texts_to_sequences([sentence])\n",
    "        pred_sentence = pad_sequences(pred_sentence, maxlen=config.maxlen)\n",
    "        pred = model.predict(pred_sentence)\n",
    "        p_test.loc[index, 'predictions'] = cefr_levels[np.argmax(pred)]\n",
    "\n",
    "\n",
    "make_predictions(model, test)\n",
    "print(f'Ended prediction at {datetime.datetime.now()}')"
   ],
   "id": "5be0f7cb8a8d321d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "y_pred = test['predictions']\n",
    "\n",
    "y_true = test['level']\n",
    "\n",
    "def map_func(x):\n",
    "    return label_to_id_mapping.get(x, -1)\n",
    "\n",
    "y_true_mapped = np.vectorize(map_func)(y_true)\n",
    "y_pred_mapped = np.vectorize(map_func)(y_pred)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_true=y_true_mapped, y_pred=y_pred_mapped)\n",
    "print(f'Accuracy: {accuracy:.3f}')\n",
    "\n",
    "# Generate accuracy report\n",
    "unique_labels = set(y_true_mapped)  # Get unique labels\n",
    "\n",
    "for label in unique_labels:\n",
    "    label_indices = [i for i in range(len(y_true_mapped)) if y_true_mapped[i] == label]\n",
    "    label_y_true = [y_true_mapped[i] for i in label_indices]\n",
    "    label_y_pred = [y_pred_mapped[i] for i in label_indices]\n",
    "    label_accuracy = accuracy_score(label_y_true, label_y_pred)\n",
    "    print(f'Accuracy for label {cefr_levels[label]}: {label_accuracy:.3f}')\n",
    "\n",
    "class_report = classification_report(y_true=y_true_mapped, y_pred=y_pred_mapped, target_names=cefr_levels, labels=list(range(len(cefr_levels))))\n",
    "class_report_dict = classification_report(y_true=y_true_mapped, y_pred=y_pred_mapped, target_names=cefr_levels, labels=list(range(len(cefr_levels))), output_dict=True)\n",
    "print('\\nClassification Report:')\n",
    "print(class_report)\n",
    "table_data = []\n",
    "\n",
    "for key, value in class_report_dict.items():\n",
    "    if isinstance(value, dict):\n",
    "        table_data.append([\n",
    "            key,\n",
    "            value.get(\"precision\", 0),\n",
    "            value.get(\"recall\", 0),\n",
    "            value.get(\"f1-score\", 0),\n",
    "            value.get(\"support\", 0)\n",
    "        ])\n",
    "    else:\n",
    "        # For accuracy, add precision and recall as 0\n",
    "        table_data.append([\n",
    "            key,\n",
    "            0,\n",
    "            0,\n",
    "            value,\n",
    "            class_report_dict[\"weighted avg\"][\"support\"]\n",
    "        ])\n",
    "\n",
    "# Generate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_true=y_true_mapped, y_pred=y_pred_mapped, labels=list(range(len(cefr_levels))))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix,\n",
    "                              display_labels=cefr_levels)\n",
    "disp.plot()\n",
    "\n",
    "\n",
    "print('\\nConfusion Matrix:')\n",
    "print(conf_matrix)\n",
    "\n",
    "wandb.log({\n",
    "    \"Class Proportions\": wandb.sklearn.plot_class_proportions(train['level'], test['level'], cefr_levels),\n",
    "    \"Confusion Matrix\": plt,\n",
    "    \"Classification Report\": wandb.Table(data=table_data, columns=['Class/Metric', 'Precision', 'Recall', 'F1-score', 'Support'])\n",
    "})"
   ],
   "id": "e8b335aa607b05b7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "wandb.finish()",
   "id": "b6f73ad484d424a7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train[['text','label','level']].to_csv('../datasets/quotes/quotes_train.csv', index=False)\n",
    "test[['text','label','level']].to_csv('../datasets/quotes/quotes_test.csv', index=False)"
   ],
   "id": "100039c565f94a9c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "2982d1242dd27da0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "c826db4500c68d18",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

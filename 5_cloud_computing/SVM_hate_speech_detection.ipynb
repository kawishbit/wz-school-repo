{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lynLvZW6MwgQ",
        "outputId": "5f19da36-d8d2-41e9-efa1-27affd20fbcb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    cowok usaha lacak perhati gue lanta remeh perh...\n",
            "1    telat tau edan sarap gue gaul cigax jifla cal ...\n",
            "2    kadang pikir percaya tuhan jatuh kali kali kad...\n",
            "3                                 tau mata sipit lihat\n",
            "4             kaum cebong kafir lihat dungu dungu haha\n",
            "5                              bani taplak kawan kawan\n",
            "6    deklarasi pilih kepala daerah aman anti hoak w...\n",
            "7    gue selesai watch aldnoah zero kampret karakt ...\n",
            "8    admin belanja po nak makan ai kepal milo ai ke...\n",
            "9                                            enak ngew\n",
            "Name: Tweet, dtype: object\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.86      0.86      1507\n",
            "           1       0.81      0.79      0.80      1117\n",
            "\n",
            "    accuracy                           0.83      2624\n",
            "   macro avg       0.83      0.83      0.83      2624\n",
            "weighted avg       0.83      0.83      0.83      2624\n",
            "\n",
            "SVM, Accuracy Score: 0.8323170731707317\n"
          ]
        }
      ],
      "source": [
        "!pip install Sastrawi --quiet\n",
        "!pip install tensorflow --quiet\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import *\n",
        "import string\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from keras_preprocessing.text import Tokenizer\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from sklearn.utils import resample\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import confusion_matrix,classification_report, accuracy_score\n",
        "from sklearn.svm import LinearSVC\n",
        "import re, io, json\n",
        "\n",
        "nltk.download(\"popular\", quiet=True)\n",
        "\n",
        "data = pd.read_csv('https://raw.githubusercontent.com/kawishbit/svm-hate-speech-id/main/clean_dataset.csv')\n",
        "data.dropna(subset=['Tweet'], how='all', inplace=True)\n",
        "data = data[['Tweet','HS']]\n",
        "data.head(10)\n",
        "\n",
        "\n",
        "alay_words = pd.read_csv('https://raw.githubusercontent.com/kawishbit/svm-hate-speech-id/main/alay.csv')\n",
        "alay_words.head(10)\n",
        "indonesian_stopwords = pd.read_csv('https://raw.githubusercontent.com/kawishbit/svm-hate-speech-id/main/stopwords.txt')\n",
        "indonesian_stopwords = indonesian_stopwords.iloc[:, 0].values.tolist()\n",
        "\n",
        "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
        "\n",
        "other_exclusions = [\"#ff\", \"ff\", \"rt\"]\n",
        "stopwords.extend(other_exclusions)\n",
        "stopwords.extend(indonesian_stopwords)\n",
        "stemmer = PorterStemmer()\n",
        "def replace_alay(tweet):\n",
        "    output = []\n",
        "    for word in tweet:\n",
        "      row = alay_words[alay_words.alay == word]\n",
        "      if row.empty:\n",
        "        output.append(word)\n",
        "      else:\n",
        "        output.append(str(row['replacement'].values[0]))\n",
        "\n",
        "    return output\n",
        "\n",
        "\n",
        "def preprocess(tweet):  \n",
        "    \n",
        "    # removal of extra spaces\n",
        "    regex_pat = re.compile(r'\\s+')\n",
        "    tweet_space = tweet.str.replace(r'\\s+', ' ', regex=True)\n",
        "\n",
        "    # removal of @name[mention]\n",
        "    regex_pat = re.compile(r'@[\\w\\-]+')\n",
        "    tweet_name = tweet_space.str.replace(r'@[\\w\\-]+', '', regex=True)\n",
        "\n",
        "    # removal of links[https://abc.com]\n",
        "    giant_url_regex =  re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
        "            '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
        "    tweets = tweet_name.str.replace('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
        "            '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', regex=True)\n",
        "    \n",
        "    # removal of punctuations and numbers\n",
        "    punc_remove = tweets.str.replace(\"[^a-zA-Z]\", \" \", regex=True)\n",
        "    # remove whitespace with a single space\n",
        "    newtweet=punc_remove.str.replace(r'\\s+', ' ', regex=True)\n",
        "    # remove leading and trailing whitespace\n",
        "    newtweet=newtweet.str.replace(r'^\\s+|\\s+?$','', regex=True)\n",
        "    # replace normal numbers with numbr\n",
        "    newtweet=newtweet.str.replace(r'\\d+(\\.\\d+)?','numbr', regex=True)\n",
        "    # removal of capitalization\n",
        "    tweet_lower = newtweet.str.lower()\n",
        "    \n",
        "    # tokenizing\n",
        "    tokenized_tweet = tweet_lower.apply(lambda x: x.split())\n",
        "\n",
        "    # removal of stopwords\n",
        "    tokenized_tweet=  tokenized_tweet.apply(lambda x: [item for item in x if item not in stopwords])\n",
        "    \n",
        "    # removal of alay words & slangs\n",
        "    tokenized_tweet = tokenized_tweet.apply(lambda x:  replace_alay(x))\n",
        "\n",
        "    # stemming of the tweets\n",
        "    tokenized_tweet = tokenized_tweet.apply(lambda x: [stemmer.stem(i) for i in x])\n",
        "\n",
        "    newList = []\n",
        "    for key,value in tokenized_tweet.items():\n",
        "      tokenized_tweet[key] = ' '.join(value)\n",
        "      tweets_p= tokenized_tweet\n",
        "    \n",
        "    return tweets_p\n",
        "\n",
        "processed_tweets = preprocess(data['Tweet'])   \n",
        "\n",
        "X = processed_tweets\n",
        "print(X.head(10))\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2),max_df=0.75, min_df=5, max_features=10000)\n",
        "\n",
        "tfidf = tfidf_vectorizer.fit_transform(X)\n",
        "tfidf\n",
        "X = tfidf\n",
        "Y = data['HS'].astype(int)\n",
        "X_train_tfidf, X_test_tfidf, y_train, y_test = train_test_split(X, Y, random_state=42, test_size=0.2)\n",
        "\n",
        "support = LinearSVC(random_state=20)\n",
        "support.fit(X_train_tfidf,y_train)\n",
        "y_preds = support.predict(X_test_tfidf)\n",
        "acc3=accuracy_score(y_test,y_preds)\n",
        "report = classification_report( y_test, y_preds )\n",
        "print(report)\n",
        "print(\"SVM, Accuracy Score:\" , acc3)"
      ]
    }
  ]
}